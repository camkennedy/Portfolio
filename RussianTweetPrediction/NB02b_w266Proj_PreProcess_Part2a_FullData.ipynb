{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W266 Russian Troll Detection Project\n",
    "\n",
    "# Data Pre-Processing Notebook, Part 2 of 2\n",
    "\n",
    "This notebook takes our initial data and performs all the preprocessing steps required for it to be trained in an ML algorithm (e.g., an LSTM neural network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import stuff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import regex as re\n",
    "from csv import reader\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set Global Options\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "Here we'll load the data that we saved as the final output of Pre-Processing notebook #1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2991009 entries, 0 to 3003480\n",
      "Data columns (total 17 columns):\n",
      "tweet_id                 int64\n",
      "text                     object\n",
      "user_id                  int64\n",
      "in_reply_to_status_id    float64\n",
      "retweeted_status_id      float64\n",
      "retweet_count            float32\n",
      "favorite_count           float32\n",
      "num_hashtags             float32\n",
      "num_urls                 float32\n",
      "num_mentions             float64\n",
      "target                   int8\n",
      "statuses_count           float64\n",
      "followers_count          float64\n",
      "friends_count            float64\n",
      "favourites_count         float64\n",
      "listed_count             float64\n",
      "lang                     object\n",
      "dtypes: float32(4), float64(8), int64(2), int8(1), object(2)\n",
      "memory usage: 900.5 MB\n"
     ]
    }
   ],
   "source": [
    "#Read from pkl file\n",
    "df_alltweets = pd.read_pickle('data/df_alltweets.pkl')\n",
    "df_alltweets.info(memory_usage='deep', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genuine Tweets: 2,787,527;  IRA Tweets: 203,482\n"
     ]
    }
   ],
   "source": [
    "#Counts\n",
    "def count_tweets_by_target(df):\n",
    "    print('Genuine Tweets: {:,};  IRA Tweets: {:,}'.format(sum(df['target'] == 0),\n",
    "                                                            sum(df['target'] == 1)))\n",
    "    \n",
    "count_tweets_by_target(df_alltweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2991009 entries, 0 to 3003480\n",
      "Data columns (total 16 columns):\n",
      "tweet_id                 int64\n",
      "text                     object\n",
      "user_id                  int64\n",
      "in_reply_to_status_id    float64\n",
      "retweeted_status_id      float64\n",
      "retweet_count            float32\n",
      "favorite_count           float32\n",
      "num_hashtags             float32\n",
      "num_urls                 float32\n",
      "num_mentions             float64\n",
      "target                   int8\n",
      "statuses_count           float64\n",
      "followers_count          float64\n",
      "friends_count            float64\n",
      "favourites_count         float64\n",
      "listed_count             float64\n",
      "dtypes: float32(4), float64(8), int64(2), int8(1), object(1)\n",
      "memory usage: 731.9 MB\n"
     ]
    }
   ],
   "source": [
    "#Drop for now, maybe add back later\n",
    "df_alltweets = df_alltweets.drop(columns=['lang'])\n",
    "df_alltweets.info(memory_usage='deep', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get out of pandas\n",
    "\n",
    "#Step 1. Text id to list\n",
    "text_list = list(df_alltweets['text'])\n",
    "\n",
    "#Step 2. Metadata columns to array (we don't yet have token lengh ... need to add later)\n",
    "meta_cols = ['retweet_count','favorite_count','num_hashtags','num_urls','num_mentions',\n",
    "             'statuses_count','followers_count','friends_count','favourites_count','listed_count']\n",
    "arr_metadata = np.array(df_alltweets[meta_cols])\n",
    "\n",
    "#Step 3. Metadata columns to array (we don't yet have token lengh ... need to add later)\n",
    "target_cols = ['target']\n",
    "arr_targetdata = np.array(df_alltweets[target_cols])\n",
    "\n",
    "#Step 4. Save other data (might use it later)\n",
    "other_data = ['tweet_id','user_id','in_reply_to_status_id','retweeted_status_id']\n",
    "arr_otherdata = np.array(df_alltweets[other_data])\n",
    "\n",
    "#Delete pandas dataframe\n",
    "del df_alltweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize and Canonicalize Tweet Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to Tokenize and Canonicalize Tweet Text\n",
    "\n",
    "\"\"\"\n",
    "Source:  https://gist.github.com/tokestermw/cb87a97113da12acb388\n",
    "\n",
    "preprocess-twitter.py\n",
    "python preprocess-twitter.py \"Some random text with #hashtags, @mentions and http://t.co/kdjfkdjf (links). :)\"\n",
    "Script for preprocessing tweets by Romain Paulus\n",
    "with small modifications by Jeffrey Pennington\n",
    "with translation to Python by Motoki Wu\n",
    "Translation of Ruby script to create features for GloVe vectors for Twitter data.\n",
    "http://nlp.stanford.edu/projects/glove/preprocess-twitter.rb\n",
    "\"\"\"\n",
    "\n",
    "FLAGS = re.MULTILINE | re.DOTALL\n",
    "\n",
    "def hashtag(text):\n",
    "    text = text.group()\n",
    "    hashtag_body = text[1:]\n",
    "    if hashtag_body.isupper():\n",
    "        result = \" {} \".format(hashtag_body.lower())\n",
    "    else:\n",
    "        result = \" \".join([\"<hashtag>\"] + re.split(r\"(?=[A-Z])\", hashtag_body, flags=FLAGS))\n",
    "    return result\n",
    "\n",
    "def allcaps(text):\n",
    "    text = text.group()\n",
    "    return text.lower() + \" <allcaps>\"\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`\\-]?\"\n",
    "\n",
    "    # function so code less repetitive\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n",
    "    text = re_sub(r\"@\\w+\", \"<user>\")\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
    "    text = re_sub(r\"/\",\" / \")\n",
    "    text = re_sub(r\"<3\",\"<heart>\")\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n",
    "    text = re_sub(r\"#\\S+\", hashtag)\n",
    "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
    "    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
    "\n",
    "    ## -- I just don't understand why the Ruby script adds <allcaps> to everything so I limited the selection.\n",
    "    # text = re_sub(r\"([^a-z0-9()<>'`\\-]){2,}\", allcaps)\n",
    "    text = re_sub(r\"([A-Z]){2,}\", allcaps)\n",
    "\n",
    "    ### ORIGINAL ###\n",
    "    #return text.lower().split()  #CK added the .split() on the end.\n",
    "    #return text.lower()\n",
    "    \n",
    "    ### NEW ###\n",
    "    output = text.lower().split()  #CK added the .split() on the end.\n",
    "    #return list(itertools.chain(*[re.split('(\\W)', x) for x in output]))  #Fails because it splits < and > in tags\n",
    "    output = list(itertools.chain(*[re.split(r'([^\\w<>])', x) for x in output]))  #Splits punctuation, keeping < and >\n",
    "    return [item for item in output if item != '']  #Removes blank strings from list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc',\n",
       " 'd',\n",
       " '.',\n",
       " 'ef',\n",
       " '<hashtag>',\n",
       " 'blah',\n",
       " '<new>',\n",
       " 'wor<smile>',\n",
       " 'user',\n",
       " ':',\n",
       " '<user>',\n",
       " ':']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test our tokenizer\n",
    "a = 'abc d.ef #blah <new> word: user: <user>:'\n",
    "c = tokenize(a)\n",
    "c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Size: 26,919,192 bytes\n",
      "Token Size: 26,791,768 bytes\n"
     ]
    }
   ],
   "source": [
    "#Make new column with tokenized, canonicalized text\n",
    "token_list = list(map(tokenize, text_list))\n",
    "\n",
    "print('Text Size: {:,} bytes'.format(sys.getsizeof(text_list)))\n",
    "print('Token Size: {:,} bytes'.format(sys.getsizeof(token_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle / Save Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(text_list))\n",
    "print(type(token_list))\n",
    "#print(type(embed_arr))\n",
    "print(type(arr_metadata))\n",
    "print(type(arr_targetdata))\n",
    "print(type(arr_otherdata))\n",
    "\n",
    "#Save Orig Text\n",
    "with open('data/text.pkl', 'wb') as fp:\n",
    "    pickle.dump(text_list, fp)\n",
    "\n",
    "#Save Tokens    \n",
    "with open('data/token.pkl', 'wb') as fp:\n",
    "    pickle.dump(token_list, fp)\n",
    "\n",
    "# #Save Embeddings\n",
    "# np.save('data/embed_arr.npy', embed_arr)\n",
    "\n",
    "#Save Metadata\n",
    "np.save('data/arr_metadata.npy', arr_metadata)\n",
    "\n",
    "#Save Targets\n",
    "np.save('data/arr_targetdata.npy', arr_targetdata)\n",
    "\n",
    "#Save Other Data\n",
    "np.save('data/arr_otherdata.npy', arr_otherdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3.0G\r\n",
      "drwxr-xr-x 3 brandon_cummings brandon_cummings 4.0K Aug  9 21:43 .\r\n",
      "drwxr-xr-x 4 brandon_cummings brandon_cummings 4.0K Aug 10 13:22 ..\r\n",
      "-rw-r--r-- 1 brandon_cummings brandon_cummings 229M Aug 10 13:23 arr_metadata.npy\r\n",
      "-rw-r--r-- 1 brandon_cummings brandon_cummings  92M Aug 10 13:23 arr_otherdata.npy\r\n",
      "-rw-r--r-- 1 brandon_cummings brandon_cummings 2.9M Aug 10 13:23 arr_targetdata.npy\r\n",
      "-rw-r--r-- 1 brandon_cummings brandon_cummings 537M Aug 10 12:57 df_alltweets.pkl\r\n",
      "-rw-r--r-- 1 brandon_cummings brandon_cummings 1.3G Aug 10 13:14 df_mini_alltweets_with_embed.pkl\r\n",
      "drwxr-xr-x 2 brandon_cummings brandon_cummings 4.0K Jul 19 12:42 GLoVE\r\n",
      "-rw-r--r-- 1 brandon_cummings brandon_cummings  528 Aug 10 13:14 GloVe_Unknown_50.npy\r\n",
      "-rw-r--r-- 1 brandon_cummings brandon_cummings 243M Aug 10 13:22 text.pkl\r\n",
      "-rw-r--r-- 1 brandon_cummings brandon_cummings 575M Aug 10 13:23 token.pkl\r\n"
     ]
    }
   ],
   "source": [
    "#Check to see that our .pkl file is there (and note its size)\n",
    "!ls ./data -lah"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
