{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# GPS Summary & Steepness Calculator\n",
    "Cameron Kennedy, Fall 2017\n",
    "\n",
    "## Notes\n",
    "This is the Jupyter Notebook used to create the program before porting it over to a command line .py file.  Though the main program is in the next cell, there's a significant amount of fiddling around with code and other thoughts below that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the GPS Summary & Steepness Calculator!\n",
      "\n",
      "***** W200 Instructors/Graders: Note, several sample GPX files for testing are located in sub-folders d1 and d2. *****\n",
      "\n",
      "\n",
      "MAIN MENU\n",
      "Currently loaded file / folder: None\n",
      "Please choose from the following options:\n",
      "  1. Load Single GPX File.\n",
      "  2. Load Multiple GPX Files.\n",
      "  3. Analyze Data.\n",
      "  4. Quit the program.\n",
      "  Enter your choice here: 2\n",
      "\n",
      "### Load Multiple GPX Files selected. ###\n",
      "Please enter a directory (i.e., folder) name. Absolute or relative path names are accepted.\n",
      "d2\n",
      "10 files loaded so far!\n",
      "20 files loaded so far!\n",
      "30 files loaded so far!\n",
      "40 files loaded so far!\n",
      "50 files loaded so far!\n",
      "60 files loaded so far!\n",
      "70 files loaded so far!\n",
      "80 files loaded so far!\n",
      "90 files loaded so far!\n",
      "100 files loaded so far!\n",
      "110 files loaded so far!\n",
      "120 files loaded so far!\n",
      "130 files loaded so far!\n",
      "140 files loaded so far!\n",
      "150 files loaded so far!\n",
      "160 files loaded so far!\n",
      "170 files loaded so far!\n",
      "180 files loaded so far!\n",
      "190 files loaded so far!\n",
      "200 files loaded so far!\n",
      "210 files loaded so far!\n",
      "220 files loaded so far!\n",
      "230 files loaded so far!\n",
      "240 files loaded so far!\n",
      "250 files loaded so far!\n",
      "260 files loaded so far!\n",
      "270 files loaded so far!\n",
      "280 files loaded so far!\n",
      "290 files loaded so far!\n",
      "300 files loaded so far!\n",
      "310 files loaded so far!\n",
      "311 total files loaded!\n",
      "\n",
      "Grouping Events ...\n",
      "Removing 20 events because they were too short or failed to load.\n",
      "You have 291 total events. \n",
      "82 are unique events (not grouped), with the remaining 209 clustered into 23 groups.\n",
      "Here are the groups of events:\n",
      "  Event Group 1: 29 events, 3.75 mi., Most recent: 11/01/17 14:05:04\n",
      "  Event Group 2: 73 events, 6.76 mi., Most recent: 10/25/17 07:39:35\n",
      "  Event Group 3: 5 events, 4.20 mi., Most recent: 10/20/17 14:11:56\n",
      "  Event Group 4: 2 events, 1.69 mi., Most recent: 10/10/17 15:05:27\n",
      "  Event Group 5: 3 events, 5.45 mi., Most recent: 09/02/17 14:19:09\n",
      "  Event Group 6: 4 events, 5.81 mi., Most recent: 08/13/17 12:21:34\n",
      "  Event Group 7: 50 events, 11.81 mi., Most recent: 07/16/17 11:01:42\n",
      "  Event Group 8: 3 events, 4.87 mi., Most recent: 04/23/17 12:57:44\n",
      "  Event Group 9: 3 events, 7.30 mi., Most recent: 04/15/17 10:30:34\n",
      "  Event Group 10: 2 events, 35.73 mi., Most recent: 11/06/16 07:51:03\n",
      "  Event Group 11: 2 events, 34.91 mi., Most recent: 10/29/16 07:26:40\n",
      "  Event Group 12: 6 events, 8.45 mi., Most recent: 10/05/16 08:14:55\n",
      "  Event Group 13: 4 events, 12.44 mi., Most recent: 10/02/16 12:46:49\n",
      "  Event Group 14: 2 events, 17.10 mi., Most recent: 06/28/16 09:22:26\n",
      "  Event Group 15: 3 events, 9.33 mi., Most recent: 06/08/16 10:15:04\n",
      "  Event Group 16: 2 events, 6.32 mi., Most recent: 11/07/15 14:56:01\n",
      "  Event Group 17: 3 events, 32.93 mi., Most recent: 08/16/14 08:44:22\n",
      "  Event Group 18: 2 events, 23.60 mi., Most recent: 07/20/13 10:28:55\n",
      "  Event Group 19: 2 events, 36.47 mi., Most recent: 06/28/13 08:19:08\n",
      "  Event Group 20: 2 events, 9.80 mi., Most recent: 06/02/13 08:41:56\n",
      "  Event Group 21: 2 events, 24.11 mi., Most recent: 05/29/13 04:42:32\n",
      "  Event Group 22: 3 events, 7.71 mi., Most recent: 08/18/12 09:42:05\n",
      "  Event Group 23: 2 events, 0.86 mi., Most recent: 06/10/12 10:00:46\n",
      "\n",
      "Please enter the number of the group you would like to analyze.\n",
      "Enter 0 to return to the main menu.\n",
      "19\n",
      "\n",
      "MAIN MENU\n",
      "Currently loaded file / folder: Multi_Event loaded, folder: d2\n",
      "Please choose from the following options:\n",
      "  1. Load Single GPX File.\n",
      "  2. Load Multiple GPX Files.\n",
      "  3. Analyze Data.\n",
      "  4. Quit the program.\n",
      "  Enter your choice here: 3\n",
      "\n",
      "### Analyze Data selected. ###\n",
      "\n",
      "Analyzing Data ...\n",
      "Analyzing Multiple Events.\n",
      "Here's a summary of this event group!\n",
      "  You have completed this event 2 times.\n",
      "  Distance: 36.47 mi.\n",
      "  Duration and Speed Summary: \n",
      "    Fastest: 06:13:30, 5.9 mph avg.\n",
      "    Median: 06:13:30, 5.9 mph avg.\n",
      "    Slowest: 06:13:30, 5.9 mph avg.\n",
      "\n",
      "  Specific events:\n",
      "    1. Date/Time: 06/28/13 08:19:08, Duration: 06:13:30, -0.0% vs. Median, -0.0% vs. Fastest.\n",
      "    2. Date/Time: 06/28/13 08:19:08, Duration: 06:13:30, -0.0% vs. Median, -0.0% vs. Fastest.\n",
      "\n",
      "MAIN MENU\n",
      "Currently loaded file / folder: Multi_Event loaded, folder: d2\n",
      "Please choose from the following options:\n",
      "  1. Load Single GPX File.\n",
      "  2. Load Multiple GPX Files.\n",
      "  3. Analyze Data.\n",
      "  4. Quit the program.\n",
      "  Enter your choice here: 4\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "## [Cameron Kennedy]\n",
    "'''### Program Overview ###\n",
    "This program will take one or multiple GPX files, and provide summary information on them that I’ve found\n",
    "difficult to obtain from existing software, specifically, both steepness analysis of a single event, and\n",
    "analysis comparing multiple events.\n",
    "\n",
    "For those unfamiliar, a GPX file (GPS Exchange Format) is an XML file generated from GPS data – often from fitness \n",
    "devices such as watches, phones, or dedicated devices (e.g., a cycling GPS) – when a person records a journey \n",
    "(typically an athletic event such as running, cycling, or hiking).  Most devices either store their data natively \n",
    "in GPX format, or can export to GPX.\n",
    "\n",
    "Complexities of this program include classes calling other classes, the inherent imprecise nature of GPS data  \n",
    "(especially with altitude data) and thus writing the necessary smoothing / approximating functions, working with the \n",
    "file system, and file sizes (GPX files I use are typically hundreds of kilobytes, which is fairly small but in \n",
    "aggregate could be a bit slow).\n",
    "\n",
    "Users are required to supply their own GPX files, though several are included for testing purposes in folders d1 and \n",
    "d2 (located in the program's main folder).'\n",
    "'''\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import os  #Used for file & directory access\n",
    "import gpxpy  #GPX parser\n",
    "#If the gpxpy package is missing, the Anaconda installation command is: conda install -c conda-forge gpxpy\n",
    "from sklearn.cluster import DBSCAN  #Used for clustering similar events\n",
    "from operator import itemgetter\n",
    "\n",
    "print('Welcome to the GPS Summary & Steepness Calculator!')\n",
    "print()\n",
    "print('***** W200 Instructors/Graders: Note, several sample GPX files for testing are located in '\n",
    "      + 'sub-folders d1 and d2. *****')\n",
    "print()\n",
    "\n",
    "class Menu:\n",
    "    '''Main menu function. Prompts the user to load a file, a folder, analyze the loaded data, or exit the program.'''\n",
    "    def __init__(self):\n",
    "        self.choice = 0\n",
    "        self.choice_errormsg = '  INPUT ERROR! Please enter a valid main menu choice: 1, 2, 3 or 4.'\n",
    "        self.data_to_analyze = None\n",
    "        self.display_menu()\n",
    "    \n",
    "    def display_menu(self):\n",
    "        '''Prints the menu, prompts users for choice, displays currently loaded file(s), and handles errors.'''\n",
    "        #Print menu\n",
    "        while self.choice != 4:\n",
    "            if str(self.data_to_analyze) == 'None':\n",
    "                self.data_to_analyze = None\n",
    "            print()\n",
    "            print('MAIN MENU')\n",
    "            print('Currently loaded file / folder: ' + str(self.data_to_analyze)) #Print loaded files, or None\n",
    "            print('Please choose from the following options:')\n",
    "            print('  1. Load Single GPX File.')\n",
    "            print('  2. Load Multiple GPX Files.')\n",
    "            print('  3. Analyze Data.')\n",
    "            print('  4. Quit the program.')\n",
    "\n",
    "            try:\n",
    "                self.choice = int(input('  Enter your choice here: '))\n",
    "            except:\n",
    "                self.choice = 0  #Resets to 0. Actual error message handled in 'else' statement below\n",
    "            \n",
    "            if self.choice == 1:\n",
    "                print('\\n### Load Single GPX File selected. ###')\n",
    "                self.load_single_file()\n",
    "            elif self.choice == 2:\n",
    "                print('\\n### Load Multiple GPX Files selected. ###')\n",
    "                self.load_multi_file()\n",
    "            elif self.choice == 3:\n",
    "                print('\\n### Analyze Data selected. ###')\n",
    "                Analyze_Data(self.data_to_analyze)\n",
    "            elif self.choice == 4:\n",
    "                print('Goodbye!')\n",
    "            else:\n",
    "                print(self.choice_errormsg)\n",
    "\n",
    "    def load_single_file(self):\n",
    "        '''Solicits then passes a filename string (valid or invalid) to the Single_Event class.'''\n",
    "        print('Please enter a file name. Absolute or relative path names are accepted.')\n",
    "        filename = str(input())\n",
    "        self.data_to_analyze = Single_Event(filename)\n",
    "        \n",
    "    def load_multi_file(self):\n",
    "        '''Solicits then passes a directory string (valid or invalid) to the Multi_Event class.\n",
    "        Directories with only 1 GPX file are convered to a Single_Event class.\n",
    "        '''\n",
    "        #For all files in folder, call Multi_Event to add them to a list.\n",
    "        print('Please enter a directory (i.e., folder) name. Absolute or relative path names are accepted.')\n",
    "        dir_name = str(input())\n",
    "        self.data_to_analyze = Multi_Event(dir_name)\n",
    "        if len(self.data_to_analyze.gpx_file_list) == 0:  #Convert to Single_Event class if only one event in list.\n",
    "            self.data_to_analyze = None\n",
    "        elif len(self.data_to_analyze.gpx_file_list) == 1:  #Convert to Single_Event class if only one event in list.\n",
    "            print('Only one GPX file found in this folder. Treating as single file (Single_Event class).')\n",
    "            self.data_to_analyze = self.data_to_analyze.gpx_file_list[0]\n",
    "\n",
    "class Single_Event:\n",
    "    '''Takes a filename (and path, optionally) and 'returns' a parsed GPX file in its self.event attribute.'''\n",
    "    def __init__(self, file_loc):\n",
    "        self.file_loc = file_loc\n",
    "        self.event = None\n",
    "        self.load_file()  #Learned __init__ methods can't return things (other than None), so calling separate method\n",
    "    \n",
    "    def load_file(self):\n",
    "        '''Loads a single file.'''\n",
    "        \n",
    "        try:\n",
    "            if self.file_loc[-4:] != '.gpx':\n",
    "                raise Exception()\n",
    "            gpx_file = open(self.file_loc, 'r')  #r for read-only\n",
    "            self.event = gpxpy.parse(gpx_file)  #This is where gpxpy does its magic of interpreting the gpx xml code.\n",
    "            self.event.adjust_time(timedelta(hours=-7))  #Offsets event times 7 hours from GMT.\n",
    "            #A future version could include local time of the event, but it doesn't appear to be stored in the GPX file.\n",
    "            gpx_file.close()\n",
    "            \n",
    "        except:\n",
    "            print('Invalid file. Please enter a valid GPX file.')\n",
    "            self.file_loc = None\n",
    "            return None\n",
    "            \n",
    "    def __str__(self):\n",
    "        if self.file_loc is not None:\n",
    "            return str('Single Event: File = ' + os.path.basename(self.file_loc))  #Print file, not full path.\n",
    "        else:\n",
    "            return 'None'\n",
    "\n",
    "class Multi_Event:\n",
    "    '''Takes a directory and 'returns' a list of multiple Single_Event objects.'''\n",
    "    def __init__(self, dir_loc):\n",
    "        self.dir_loc = dir_loc\n",
    "        self.gpx_file_list = []\n",
    "        self.load_dir()\n",
    "        \n",
    "    def load_dir(self):\n",
    "        '''Checks to ensure a valid directory is passed in, and if so, loops through every file to include all\n",
    "        .gpx files as Single_Event objects in a list. Also sorts the list by event time, in descending order.\n",
    "        '''\n",
    "        if os.path.isdir(self.dir_loc): #Check if valid folder\n",
    "            i = 0\n",
    "            for fn in os.listdir(self.dir_loc):  #Loop through items in folder\n",
    "                if fn.endswith('.gpx'):  #If it's a GPX file ...\n",
    "                    i += 1\n",
    "                    #Makes list of Single_Event objects by calling the Single_Event class.\n",
    "                    try:\n",
    "                        self.gpx_file_list.append(Single_Event(os.path.abspath(self.dir_loc) + '\\\\' + fn))\n",
    "                    except:\n",
    "                        print('BOGUS GPX FILE!')\n",
    "                        i -= 1\n",
    "                        continue\n",
    "                    if i % 10 == 0:  #Give status every 10 files.\n",
    "                        print('{} files loaded so far!'.format(i))\n",
    "            print('{} total files loaded!'.format(i))\n",
    "            if len(self.gpx_file_list) == 0:\n",
    "                print('No GPX files found in this folder. Please choose a different folder.')\n",
    "                return None\n",
    "            else:  #Learned nifty way to sort list by attribute of object in list!\n",
    "                self.gpx_file_list.sort(key=lambda obj:obj.event.time, reverse=True)\n",
    "                self.group_like_events()\n",
    "                \n",
    "        else:\n",
    "            print('Invalid directory selected. Please enter a valid directory. Returning to Main Menu.')\n",
    "            self.dir_loc = None\n",
    "            return None\n",
    "         \n",
    "    def __str__(self):\n",
    "        return str('Multi_Event loaded, folder: ' + self.dir_loc)\n",
    "            \n",
    "    def group_like_events(self):\n",
    "        '''Clusters like events together.  Then prompts the user with a list of events from which to choose.\n",
    "        Then reduces the Multi_Event list (self.gpx_file_list) to only the events in that group.\n",
    "        '''\n",
    "        print('\\nGrouping Events ...')\n",
    "        \n",
    "        #Build array of start_lat, start_lon, end_lat, end_lon, dist\n",
    "        data_to_cluster = []\n",
    "        items_to_remove = []\n",
    "        i = -1\n",
    "        event_dist_scale = 100000  #Used to scale event distance when clustering.\n",
    "        for SE in self.gpx_file_list:  #SE stands for an instance of the Single_Event class.\n",
    "            distance = 0\n",
    "            duration = 0\n",
    "            i += 1\n",
    "            try:  #These occasionally fail, so wrapping them in a try statement and eliminating event upon failure.\n",
    "                distance = Conversions(SE.event.length_2d()).meters_to_miles()\n",
    "                duration = SE.event.get_duration()\n",
    "            except:\n",
    "                items_to_remove.append(i)\n",
    "                continue\n",
    "            if distance < 0.25 or duration < (5 * 60):  #Exclude events < 0.25 miles or < 5 min.\n",
    "                items_to_remove.append(i)                \n",
    "            else:\n",
    "                data_to_cluster.append([SE.event.length_2d() / event_dist_scale,  \n",
    "                                            #Convert meters to smaller scale for clustering\n",
    "                                        SE.event.get_points_data()[0].point.latitude,\n",
    "                                        SE.event.get_points_data()[0].point.longitude,\n",
    "                                        SE.event.get_points_data()[-1].point.latitude,\n",
    "                                        SE.event.get_points_data()[-1].point.longitude,])\n",
    "        print('Removing {} events because they were too short or failed to load.'.format(len(items_to_remove)))\n",
    "        if items_to_remove != []:\n",
    "            items_to_keep = set(range(len(self.gpx_file_list))) - set(items_to_remove)\n",
    "            self.gpx_file_list = itemgetter(*items_to_keep)(self.gpx_file_list)\n",
    "        \n",
    "        if len(data_to_cluster) == 0:\n",
    "            print('The selected directory does contains no valid gpx files. Nothing to analyze. Returning to Main Menu.')\n",
    "            self.gpx_file_list = []\n",
    "        else:\n",
    "            db = DBSCAN(eps=0.005, min_samples=2).fit(np.array(data_to_cluster)) #Clustering algorithm\n",
    "            '''This is where the clustering \"magic\" happens.  The eps variable took a fair amount of tweaking to\n",
    "            get right, along with manually scaling the distance variable to be a similar (roughly) scale as the\n",
    "            lat & lon variables.  This approach is admittedly not perfect, but it works.  Also, this clustering\n",
    "            algorithm will fail near the North and South Poles due to converging longitudes; I can live with that.\n",
    "            Set min_samples=2 to allow for groups as small as 2 events.\n",
    "            '''\n",
    "\n",
    "            labels = list(db.labels_)  #List of all events and their groups.\n",
    "                #Makes a list of events from 0 to groups_count. -1 indicates an individual event (unique, not clusetered).\n",
    "            indiv_events_count = labels.count(-1)\n",
    "            total_events_count = len(labels)\n",
    "            groups_count = max(labels) + 1\n",
    "            print('You have {} total events. '.format(len(labels)))\n",
    "            if groups_count == 0:\n",
    "                print('None of them are the same event. Nothing to analyze. Returning to Main Menu.')\n",
    "                self.gpx_file_list = []\n",
    "            else:\n",
    "                print('{} are unique events (not grouped), with the remaining '.format(indiv_events_count)\n",
    "                      + '{} clustered into {} groups.'.format(total_events_count - indiv_events_count, groups_count)\n",
    "                     )\n",
    "                print('Here are the groups of events:')\n",
    "\n",
    "                for i in range(groups_count):\n",
    "                    distance = Conversions(data_to_cluster[labels.index(i)][0] * event_dist_scale).meters_to_miles()\n",
    "                    print('  Event Group {}: {} events, {:.2f} mi., '.format(i+1, labels.count(i), distance)\n",
    "                          + 'Most recent: {:%D %H:%M:%S}'.format(self.gpx_file_list[labels.index(i)].event.get_time_bounds()[0])\n",
    "                         )\n",
    "\n",
    "                input_text = ('Please enter the number of the group you would like to analyze.\\n' +\n",
    "                             'Enter 0 to return to the main menu.')\n",
    "                print('\\n' + input_text)\n",
    "                #I considered an option to analyze all events, but the analysis becomes nonsensical, so I opted against it.\n",
    "                \n",
    "                #Whittle down list of events to only those in selected groups:\n",
    "                x = None\n",
    "                while x not in range(groups_count + 1):\n",
    "                    try:\n",
    "                        x = int(input())\n",
    "                        if x == 0:\n",
    "                            self.gpx_file_list = []\n",
    "                        elif 1 <= x <= groups_count:                            \n",
    "                            indicies = [index for index, value in enumerate(labels) if value == x-1]\n",
    "                            #* lets itemgetter accept a list.\n",
    "                            self.gpx_file_list = itemgetter(*indicies)(self.gpx_file_list)  \n",
    "                        else:\n",
    "                            raise Exception()\n",
    "                    except:\n",
    "                        x = None\n",
    "                        print('\\nInvalid input. ' + input_text)\n",
    "        \n",
    "class Analyze_Data:\n",
    "    '''Takes a Single_Event or Multi_Event object as input.  For Single_Event objects, it prints summary statistics\n",
    "    including a steepness breakdown.  For Multi_Event objects, it prints summary statistics along with a listing\n",
    "    of each event, including a comparison to the best and median times for that event. \n",
    "    '''\n",
    "    def __init__(self, obj_to_analyze):\n",
    "        print('\\nAnalyzing Data ...')\n",
    " \n",
    "        self.obj_to_analyze = obj_to_analyze\n",
    "        \n",
    "        if isinstance(self.obj_to_analyze, Single_Event):\n",
    "            self.analyze_single()\n",
    "        elif isinstance(self.obj_to_analyze, Multi_Event):\n",
    "            self.analyze_multi()\n",
    "        else:\n",
    "            print('No data found or wrong type of data loaded. Please reload data and try again.')\n",
    "\n",
    "    def analyze_single(self):\n",
    "        '''Analyze a Single_Event instance. Calculates summary stats and steepness bands.'''\n",
    "        print('Analyzing Single Event.')\n",
    "        event = self.obj_to_analyze.event\n",
    "        \n",
    "        #CALCULATE SUMMARY STATS.\n",
    "        #Grab summary stats\n",
    "        start_end_times = event.get_time_bounds()\n",
    "        duration = event.get_duration()\n",
    "        distance = Conversions(event.length_2d()).meters_to_miles()\n",
    "        asc_des = event.tracks[0].get_uphill_downhill()\n",
    "        asc = Conversions(asc_des[0]).meters_to_feet()\n",
    "        des = Conversions(asc_des[1]).meters_to_feet()\n",
    "        \n",
    "        #Print summary stats\n",
    "        print('Here\\'s a summary of your event.')        \n",
    "        print('  Start Date & Time: {:%D %H:%M:%S}.'.format(start_end_times[0]))\n",
    "        print('  End Date & Time: {:%D %H:%M:%S}.'.format(start_end_times[1]))\n",
    "        print('  Duration: {:%H:%M:%S}.'.format(Conversions(duration).sec_to_datetime()))\n",
    "        print('  Distance: {:.2f} miles.'.format(distance))\n",
    "        print('  Average Speed: {:.1f} mph.'.format(distance / (duration/3600)))\n",
    "        print('  Elevation Ascent / Descent: {:,.0f} ft. / {:,.0f} ft.'.format(asc, des))\n",
    "        \n",
    "        #CALCULATE STEEPNESS GRADES\n",
    "        #Bucket into 10 groups, symmetrically distributed around 0.\n",
    "        dists = []\n",
    "        grades = []\n",
    "        smooth_dist_thresh = 5  #Length in meters.\n",
    "        '''Had to tweak this several times to get a good value for smoothing.'''\n",
    "\n",
    "        #Grab points\n",
    "        '''Makes a new list of points that are at least smooth_dist_thresh apart.'''\n",
    "        prev_item = event.get_points_data()[0]\n",
    "        for item in event.get_points_data():\n",
    "            if item.point.distance_2d(prev_item.point) > smooth_dist_thresh:  #Serves as a smoothing function.\n",
    "                distance = item.point.distance_2d(prev_item.point)\n",
    "                dists.append(distance)\n",
    "                \n",
    "                #Calculate all grades\n",
    "                ele_change = item.point.elevation - prev_item.point.elevation\n",
    "                if distance > 0:\n",
    "                    grades.append(ele_change / distance)\n",
    "                else:\n",
    "                    grades.append(0)\n",
    "                prev_item = item\n",
    "        \n",
    "        #Also a smoothing component, to remove the most extreme grades (since they're typically erroneous).\n",
    "        #Gets the indicies of the top and bottom grades, then removes those items from both grades and lists.\n",
    "        pct_thresh = 5\n",
    "        lower_pct = np.percentile(grades, pct_thresh)\n",
    "        upper_pct = np.percentile(grades, 100 - pct_thresh)\n",
    "        indices_to_omit = []\n",
    "        i = 0        \n",
    "        for item in grades:\n",
    "            if item > upper_pct or item < lower_pct:\n",
    "                indices_to_omit.append(i)\n",
    "            i += 1\n",
    "        \n",
    "        items_to_keep = set(range(len(grades))) - set(indices_to_omit)\n",
    "        grades = itemgetter(*items_to_keep)(grades)\n",
    "        dists = itemgetter(*items_to_keep)(dists)\n",
    "        \n",
    "        if len(dists) != len(grades):\n",
    "            print('WARNING! Distance and Grade list lengths aren\\'t the same.')\n",
    "    \n",
    "        #Get min/max grade; calculate upper grade boundary (5%, 10%, 15%, 20%, etc.; use absolute grade)\n",
    "        max_abs_grade = abs(max(grades, key=abs))\n",
    "        #Round to the nearest 0.05 higher than highest absolute grade\n",
    "        max_grade_bound = (((max_abs_grade*100)//5)*5 + 5)/100\n",
    "        grade_incr = max_grade_bound / 5\n",
    "        \n",
    "        #Make list of 10 lists.\n",
    "        dists_by_grade = []\n",
    "        [dists_by_grade.append([]) for dummy in range(10)]\n",
    "            \n",
    "        #Assign distances to buckets. Loop through grades; determine which index (0-9); apply index for dists\n",
    "        i = 0\n",
    "        for grade in grades:\n",
    "            #Math to determine bucket; better than looping\n",
    "            index = int(10 - 10*(grade+max_grade_bound)/(max_grade_bound*2))\n",
    "            dists_by_grade[index].append(dists[i])\n",
    "            i += 1\n",
    "        \n",
    "        dist_grades = sum(sum(i) if isinstance(i, list) else i for i in dists_by_grade) #Quickly sums 2-level list.\n",
    "        '''Important to sum distance associated with grades and use that to calc %'s.\n",
    "        It'll vary slightly vs. actual event distance. That's okay because we're getting the percentage of grades,\n",
    "        so the variation in distances is negligible. But the %'s won't add up if using event distance.       \n",
    "        '''\n",
    "                \n",
    "        #Scales grade output back to actual distance travelled\n",
    "        scale_factor = event.length_2d() / dist_grades\n",
    "        \n",
    "        print('  Steepness Breakdown:')\n",
    "        for i in range(10):\n",
    "            print('    {:+.0%} to {:+.0%} grade: '.format(max_grade_bound-grade_incr, max_grade_bound), end='')\n",
    "            print('{:.2f} mi., '.format(Conversions(sum(dists_by_grade[i])).meters_to_miles()*scale_factor), end='')\n",
    "            print('{:.0%} of distance.'.format(sum(dists_by_grade[i]) / dist_grades))\n",
    "            max_grade_bound -= grade_incr\n",
    "        \n",
    "    def analyze_multi(self):\n",
    "        '''Prints summary statistics and comparisons of multiple events in the selected group.'''\n",
    "        print('Analyzing Multiple Events.')\n",
    "        events = self.obj_to_analyze.gpx_file_list\n",
    "        \n",
    "        durations_list = []\n",
    "        speeds_list = []\n",
    "        \n",
    "        for SE in events:  #SE stands for an instance of the Single_Event class\n",
    "            durations_list.append(SE.event.get_duration())\n",
    "            \n",
    "        distance = Conversions(events[0].event.length_2d()).meters_to_miles()\n",
    "        dur_min = min(durations_list)\n",
    "        dur_med = np.median(durations_list)\n",
    "        dur_max = max(durations_list)\n",
    "        dur_fastest = Conversions(dur_min).sec_to_datetime()\n",
    "        dur_median = Conversions(dur_med).sec_to_datetime()\n",
    "        dur_slowest = Conversions(dur_max).sec_to_datetime()\n",
    "        speed_fastest = distance / (dur_min/3600)\n",
    "        speed_median = distance / (dur_med/3600)\n",
    "        speed_slowest = distance / (dur_max/3600)\n",
    "        \n",
    "        print('Here\\'s a summary of this event group!')\n",
    "        print('  You have completed this event {} times.'.format(len(events)))\n",
    "        print('  Distance: {:.2f} mi.'.format(distance))\n",
    "        print('  Duration and Speed Summary: ')\n",
    "        print('    Fastest: {:%H:%M:%S}, {:.1f} mph avg.'.format(dur_fastest, speed_fastest))\n",
    "        print('    Median: {:%H:%M:%S}, {:.1f} mph avg.'.format(dur_median, speed_median))\n",
    "        print('    Slowest: {:%H:%M:%S}, {:.1f} mph avg.'.format(dur_slowest, speed_slowest))\n",
    "        print()\n",
    "        print('  Specific events:')\n",
    "        \n",
    "        #Remember, list is already sorted from most recent to least recent\n",
    "        i = 0\n",
    "        for SE in events:  #SE stands for an instance of the Single_Event class\n",
    "            i += 1\n",
    "            dur_event = durations_list[i-1]\n",
    "            dur_vs_median = ((dur_event / dur_med) - 1) * (-1)\n",
    "            dur_vs_best = ((dur_event / dur_min) - 1) * (-1)\n",
    "            print('    {}. Date/Time: {:%D %H:%M:%S}'.format(i, SE.event.time)\n",
    "                  + ', Duration: {:%H:%M:%S}, '.format(Conversions(dur_event).sec_to_datetime())\n",
    "                  + '{:+.1%} vs. Median, {:+.1%} vs. Fastest.'.format(dur_vs_median, dur_vs_best)\n",
    "                 )\n",
    "\n",
    "class Conversions:\n",
    "    '''A handful of conversions used across different classes.'''\n",
    "    def __init__(self, value_in):\n",
    "        self.value_in = value_in\n",
    "    \n",
    "    def meters_to_miles(self):\n",
    "        return self.value_in * 0.000621371\n",
    "    \n",
    "    def meters_to_feet(self):\n",
    "        return self.value_in * 3.28084\n",
    "    \n",
    "    def sec_to_datetime(self):\n",
    "        '''Converts an integer of seconds to a datetime object.'''\n",
    "        return datetime(1,1,1) + timedelta(seconds=int(self.value_in))\n",
    "\n",
    "Menu();  #Run it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punch List\n",
    "## Definitely Implement\n",
    "- DONE Format feet and miles to 0 and 2 decimals, respectively. Elsewhere too.\n",
    "- DONE Fix error after entering an invalid file in Option 1 (might occur in Option 2 also).\n",
    "- DONE Clean up debugging print statements, e.g., printing when a function runs.\n",
    "- DONE Add spacing before / after printouts.\n",
    "- DONE Make PDF of Reflections (Observations).  1 page.\n",
    "- DONE Must run as .py file from the command line.\n",
    "- Sync Design Doc v02\n",
    "- DONE Review code, line by line.\n",
    "- DONE Final preparation:\n",
    "  - DONE Copy to .py file and test.\n",
    "  - DONE Push to github.\n",
    "- Get Out of Scope items from the design document.\n",
    "- DONE Find optimal eps value for clustering.\n",
    "  - DONE Play with various values\n",
    "  - DONE Scale distance\n",
    "- DONE Put more comments in the code.\n",
    "- DONE S&R for GMT.\n",
    "- DONE Change \"individual\" to \"unique\".\n",
    "- DONE (It was correct without needing a fix.) Fix percentage errors (invert?).\n",
    "- Testing\n",
    "  - DONE Put non-GPX files in the folder d1\n",
    "  - DONE Test thoroughly for other errors  \n",
    "  - DONE Zero length / duration rides, to the extent they cause errors\n",
    "  - DONE Use multiple GPS devices, to account for devices that might produce different types of GPX files.\n",
    "  - DONE Test case with no groups (all indiv. files).  Can set eps to 0.01 to test this case.\n",
    "\n",
    "## Probably Implement\n",
    "- DONE Include % of mileage in steepness bands\n",
    "- DONE (Bad idea; doesn't work). Consider normalizing (mean=0, sd=1) clustering variables (lat, lon, dist).\n",
    "- DONE Either change the threshold for smoothing, or change to moving average. We're still getting some wonky-high grades.\n",
    "\n",
    "## Implement, Time Permitting (Nice to Have, likely out of scope)\n",
    "- V2.0 When user selects '.' folder, print the actual folder name.  Consider printing the full path for all cases.\n",
    "- V2.0 Multi track / multi segment aggregation.\n",
    "- DONE Time zone.  Investigate if gpx files / gpxpy has a local time offset. Seems plausible they would.\n",
    "- V2.0 Events with stopped time (e.g., when the user pauses the GPS).\n",
    "- V2.0 Sum time and % time within steepness bands.\n",
    "- V2.0 In Multi_Event, calculate average distance instead of picking the first event.  This change will be of negligible impact.\n",
    "- DONE Option to \"go back to main menu\" at all prompts.\n",
    "- V2.0 If reloading the same folder, skip reload.\n",
    "- Plotting\n",
    "  - Plot Color Steepness Map\n",
    "  - Graphing multi-event analysis (e.g., bar charts of steepness or event durations)\n",
    "\n",
    "## Out of Scope\n",
    "- Make variable number of bands (not going to happen).\n",
    "- Don't try to handle bad GPX files (unless gpxpy can do this easily).\n",
    "- File system / naming conventions outside Windows.  Program might work, but it migth not.\n",
    "- Testing with non-Garmin GPS devices.\n",
    "- Optimizing clustering algorithm features to hone into specific thresholds of start / end and distance separation parameters.\n",
    "- Automated comparisons across different events / different groups (the user can run it for different events).\n",
    "- Window pop-up for selecting files / folders (e.g., like you’d have in Microsoft Word)\n",
    "- Landmarks / waypoints\n",
    "- Ability for users to create custom groups of events (other than their existing ability to include only the GPX files in a folder they want analyzed)\n",
    "- Date filter of events in groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "- I have a greater appreciation for why software is released iteratively. The wish list builds quickly!  I feel like I could never stop making enhancements to add cool new functionality.\n",
    "- So much of code / time dedicated to edge cases, formatting, error handling, etc. Less to core functionality.\n",
    "- Learned quite a bit about using another library (gpxpy).  Not well documented, but sufficiently intuitive.  I spent quite a bit of time learning the attributes and methods of this library.\n",
    "- Working with the file system through the os library seemed much more challenging than it should have been.  I struggled mightily with os.path.isfile().\n",
    "- Practially, N size is not a large factor, because a single athlete can only have a finite number of events.  Imagine a 'worst case' scenario, where an athlete records one event per day for 50 years; they'd have 18,262 events, a relatively small N.  So even though the clustering algorithm is O(N^2), it will make little real-world difference.  If this algorithm were used on groups of athletes, e.g., Garmin's user base, then N would be much larger, likely necessitating a different clustering algorithm.\n",
    "- As an aspiring data scientist, I took this as an opportunity to learn about clustering algorithms, and I loved it! I knew a little about K-Means prior to this exercise (enough to know it wouldn't work because it requires the nubmer of groups to be specified, and this program needs an unspecified number of events), but nothing about other ways to form clusters.  I chose DBSCAN because it appeared to work the best, based on its description and confirmed with my repository of 310 GPS files. It's possible it wouldn't work in every case for other users, but it performed well for me, and since I've already put in several hours researching it, and because this is a programming class rather than a machine learning class, I consider it sufficient.  I also saw no slow down.\n",
    "- I wish I knew of a way to load files in parallel (perhaps to separate lists which I could then easily merge).  It takes about a while to load 300 files, and I'd love to cut that down by using all 4 cores on my machine.\n",
    "- Interesting to discover and learn how scaling negatively affected (i.e., broke) my clustering algorithm with a group of GPX files that had nearly identical components (e.g., several events that had different distances, but which started and stopped from essentially the same point).  This led me to manually scale my data.\n",
    "- It's been great to learn a bunch of new things, such as:\n",
    "  - Sorting a list of objects by one of their attributes\n",
    "  - The Conversions class I used, and passing values into it\n",
    "  - Working with the file system, and using relative and absolute file / folder names\n",
    "  - Even little things, like how *list returns list elements without the list (e.g., for arguments of itemgetter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eps Results\n",
    "Results on initial 6 events\n",
    "- 0.1: Failed. Too small, same events not clustered\n",
    "- 0.2: Worked?\n",
    "- 0.5: Worked?\n",
    "- 0.6: Clustered two more events together, with distances of 4.43 and 4.56 km. 0.13 km difference.\n",
    "- 0.7: Failed. Every event clustered together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doodling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Doodling\n",
    "import os\n",
    "ftc = \"sample01.gpx\"\n",
    "print(os.path.isdir(ftc))\n",
    "print(os.path.exists(ftc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Keep this code, it works! Example of passing a class and its attributes to other classes.\n",
    "import os  #Used for file & directory access\n",
    "import gpxpy\n",
    "\n",
    "class First:\n",
    "    def __init__(self):\n",
    "        print('Running First class')\n",
    "        self.data_to_analyze = Fetch('s.gpx')\n",
    "        print(type(self.data_to_analyze))\n",
    "        print(type(self.data_to_analyze.event))\n",
    "        Analyze(self.data_to_analyze)\n",
    "        \n",
    "class Fetch:\n",
    "    def __init__(self, file_loc):\n",
    "        self.file_loc = file_loc\n",
    "        self.load_file()  #__init__ methods don't return things (other than None), so calling separate method\n",
    "    \n",
    "    def load_file(self):\n",
    "        gpx_file = open(self.file_loc, 'r')  #r for read-only\n",
    "        self.event = gpxpy.parse(gpx_file)\n",
    "\n",
    "class Analyze:\n",
    "    def __init__(self, fetch_in):\n",
    "        print('Running Analyze class')\n",
    "        print(type(fetch_in))\n",
    "        print(type(fetch_in.event))\n",
    "        print(fetch_in.event.get_duration())\n",
    "\n",
    "First();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert Seconds to D:H:M:S\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def GetTime(sec_in):\n",
    "    sec = timedelta(seconds=sec_in)\n",
    "    d = datetime(1,1,1) + sec\n",
    "\n",
    "    print(\"DAYS:HOURS:MIN:SEC\")\n",
    "    print(\"%d:%d:%d:%d\" % (d.day-1, d.hour, d.minute, d.second))\n",
    "    \n",
    "GetTime(99999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Here\\'s a summary of your event.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calc for how to get grade bands\n",
    "((20//5)*5 + 5)/100  #Round to the nearest 0.05 higher than highest absolute grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get abs value of items in a list.  Used to get max steepness, whether ascending or descending.\n",
    "abs(max([-9.4, 1, 2, -3, -4.5, 4.5], key=abs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function # one liner to sum nested lists.\n",
    "L = ([[1, 1, 1], [1, [1, 1]], 1])\n",
    "def list_sum(L):\n",
    "    'Recursion enables this to work for any number of nest levels.'\n",
    "    total = 0  \n",
    "    for i in L:\n",
    "        if isinstance(i, list): \n",
    "            total += list_sum(i)\n",
    "        else:\n",
    "            total += i\n",
    "    return total\n",
    "\n",
    "print(list_sum(L))\n",
    "# sum(sum(i) if isinstance(i, list) else i for i in L)  #Only works for 2 level lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Median function\n",
    "from numpy import median\n",
    "print(median([1, 2, 3, 4.7])) #Lots of values\n",
    "print(median([4.7, 5.4])) #Two values\n",
    "print(median([4.7])) #Single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "x = 'C:/Users/camke/OneDrive/Education/UCB/W200_Python/CKW200GitRepo/assignments_upstream_fall17/SUBMISSIONS/Project_01/data/abc.gpx'\n",
    "print(os.path.isfile(x))\n",
    "\n",
    "try:\n",
    "    print('hi')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myList = [1,2,3,4]\n",
    "print(myList)\n",
    "print(*myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "30 % 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Practice string formatting\n",
    "print('blah blah {a:+.2f} more blah {b:+.1f}'.format(b=123, a=-456)) #Remember for +/-\n",
    "print('blah blah {a:+.0%} more blah {b:+.1%}'.format(b=-0.345, a=1.234)) #Percentages\n",
    "print('blah blah {a:+0} more blah {b:+.1}'.format(b=-0.345, a=1.234)) #Decimals\n",
    "print('    {:.0f} to {:.2f}'.format(0.1, 0.2), end=': ')\n",
    "print('next')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 8, 10, 11]\n"
     ]
    }
   ],
   "source": [
    "#Find percentile\n",
    "import numpy as np\n",
    "myList = [1, 2, 3, 4, 60, 70, 80, 99, 100, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009]\n",
    "np.random.shuffle(myList)\n",
    "upper = np.percentile(myList, 90)\n",
    "lower = np.percentile(myList, 10)\n",
    "indices_to_omit = []\n",
    "i = 0\n",
    "for item in myList:    \n",
    "    if item > upper or item < lower:\n",
    "        indices_to_omit.append(i)\n",
    "    i += 1\n",
    "print(indices_to_omit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".gpx\n"
     ]
    }
   ],
   "source": [
    "x = 'blah.gpx'\n",
    "print(x[-4:])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
