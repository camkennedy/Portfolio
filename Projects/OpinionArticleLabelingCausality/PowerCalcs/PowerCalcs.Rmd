---
title: "Power Calculations for Opinion Labeling in News Articles"
subtitle: "W241, Experiments and Causality | Spring 2019 | Final Project"
author: "Authors ... "
output:
  pdf_document: default
  html_document: default
---


```{r, results='hide'} 
# load packages 
library(data.table)
library(MASS)
library(ggplot2)

```

# Introduction

This notebook is designed to test the statistical power of the experiment. These calculations help set expectations about the chances that a statistically significant effect will be detected, if one exists. This work seeks to measure statistical power by altering sample size, treatment effect size, and treatment effect distribution.

The results found here will help the experiment in two ways:  First, it will inform the experimental design by influencing the N-sizes needed to detect an effect if one exists. Second, these results will provide context to the conclusions of the overall experiment, especially in cases where statistical significance is not found, by helping separate the possibility that an effect wasn't found because actually wasn't one vs. an effect not being found because the statistical power was low. Both of these outcomes will be applied to the overall experiment design and findings, as well as to those within blocks, and therefore with smaller sample sizes.

# Testing Nulls

We'll begin by testing the power calculation on a null hypothesis by simulating idential distirbutions of responses, repeating this experiment multiple times (re-randomizing), and then calculating the percent of experiments where the p value is less than 0.05, indicating the frequency with which we observed statistical significance by random chance as opposed to an actual effect (false positives).  We expect a result near 5%.

First, we'll generate data:

```{r}
### GENERATE DATA ###
# Create survey category options
vecPolBias <- c('1_Very Liberal','2_Somewhat Liberal','3_Slightly Liberal',
                '4_Neutral','5_Slightly Conservative','6_Somewhat Conservative',
                '7_Very Conservative')
# The numbers preceeding the response options preserving ordinality by
# forcing alpha-numeric order.
vecPolBias

#Dynamically retrieve number of categories, and also convert to a vector
intNumCategories <- length(vecPolBias)
intNumCategories
vecCategoryNums <- seq(intNumCategories)
vecCategoryNums

# #Convert to data table
# dtOptions <- data.table('BiasNum'=vecCategoryNums, 'BiasRating'=vecPolBias)
# dtOptions
# 
# #Note, our bias number is just used in data generation.
# #It is not meant to imply linearity between categories,
# #as doing so would defeat their ordinal-only nature.

# Set our probabilities of selecting responses

#We'll start with a uniform distribution (equal odds of selecting any response)
#Set probs for being in CONTROL GROUP
vecProbC <- rep(1/intNumCategories, intNumCategories) #Control probs.
vecProbC

#Set probs for being in TREATMENT GROUP
vecProbT <- vecProbC  #Treatment probs. Set the same, identical distributions
vecProbT

#Now let's run a quick observational test to see if we can generate random uniform choices
#... commenting out
#sample(x=vecPolBias, size=10, prob=vecProbC, replace=TRUE)
#sample(x=vecPolBias, size=10, prob=vecProbT, replace=TRUE)

#Let's further test by generating a larger sample and observing their frequency
intSampSize <- 100000
vecSampleDataC <- sample(x=vecPolBias, size=intSampSize, prob=vecProbC, replace=TRUE)
vecSampleDataT <- sample(x=vecPolBias, size=intSampSize, prob=vecProbT, replace=TRUE)
vecSampleData <- c(vecSampleDataC, vecSampleDataT)

dt <- data.table('outcome'=vecSampleData, 
                 'treat'=c(rep(0, intSampSize), rep(1, intSampSize))
                 )
dt[, .N / intSampSize, keyby=.(treat,outcome)]
#Yup, it works!

#Quick peek at our data ... looks good ... commenting out
#dt[sample(.N,10)]

#Finally, we'll make a function so we can repeatedly randomly generate this data
GenRandData <- function(intSampSize, vecProbC, vecProbT) { 
  
  vecSampleDataC <- sample(x=vecPolBias, size=intSampSize, prob=vecProbC, replace=TRUE)
  vecSampleDataT <- sample(x=vecPolBias, size=intSampSize, prob=vecProbT, replace=TRUE)
  vecSampleData <- c(vecSampleDataC, vecSampleDataT)
  
  return(data.table('outcome'=vecSampleData, 
                    'treat'=c(rep(0, intSampSize), rep(1, intSampSize))
                    )
         )
}

#Test our function ... it works ... commenting out
#test <- GenRandData(intSampSize, vecProbC, vecProbT)
#test[sample(.N,10)]

```

Now that we have our data, let's conduct our power tests.

```{r}
### POWER TESTS ###

#We'll first generate a model along with some summary stats
#that eventually results in a p-Value.
intSampSize = 50000  #Very large sample produces consistent model
dt <- GenRandData(intSampSize, vecProbC, vecProbT)
m <- polr(as.factor(outcome) ~ treat, data = dt, Hess=TRUE)
summary(m)
tValue <- coef(summary(m))['treat', 't value']
tValue
pValue <- pnorm(abs(tValue), lower.tail=F) * 2
pValue
pValue < 0.05
```

Now let's replicate this function to see how often it returns positive results.

```{r}
#Now we'll wrap the above stuff in a function so we can replicate it
RepPVals <- function(intSampSize, vecProbC, vecProbT, pThresh=0.05) { 

  dt <- GenRandData(intSampSize, vecProbC, vecProbT)
  m <- polr(as.factor(outcome) ~ treat, data = dt, Hess=TRUE)
  tValue <- coef(summary(m))['treat', 't value']
  pValue <- pnorm(abs(tValue), lower.tail=F) * 2
  
  #return(pValue)
  return(pValue < pThresh)

}

#Test the function
intSampSize = 500
RepPVals(intSampSize, vecProbC, vecProbT, pThresh=0.05)

#Now we replicate the function a bunch of times!
intSampSize = 500
intNumReps <- 1000
pValsNull <- replicate(intNumReps, RepPVals(intSampSize, vecProbC, vecProbT, pThresh=0.05))
numDetected <- sum(pValsNull) / intNumReps
numDetected
```

Great! Having run this a few times, we verify that we get values close to 0.05 as expected. This exercise also gives us confidence that our p-value testing setup works as expected.

# Testing Treatment Effects Against a Uniform Distribution

Now let's move on to rerunning this analysis when their actually _is_ a difference between treatment and control. To generate this difference, we'll alter the current random uniform distirbution for the treatment group, while keeping the control group the same, thus simulating a causal effect of subjects in the treatment group picking different choices.

Aside, we acknoweldge that starting from a random uniform distribution (equal probability of picking any response) for the control group is an assumption that may or may not be valid.  We'll keep this assuption for now, but later on in this analysis we'll change it to see how the results change.

## Uniform Shifted Response

First, we'll simulate a situation where we imagine every respondent has an X% probability of picking the next higher choice, and we'll refer to this change as a _uniform shift_. In this case, it means they'll shift to become more conservative, however, because our distribution is symmetric, it would work equally well or poorly in the liberal direction. Currently, every resopnse has a roughly 14% chance of being selected. Making this change will actually only result in the lowest response (Very Liberal) going down, and the highest response (Very Conservative) going up, because all the responses in the middle will have the same number of hypothetical respondents leave the for the next higher choice as they have that newly enter their choice from the lower response.  Let's code this distribution now.

```{r}
### UNIFORM SHIFTED DISTRIBUTION, GENERATION ###

#Uniform Shifted Responses
numShiftFactor <- 0.05
vecProbT <- vecProbC + c(-numShiftFactor,0,0,0,0,0,numShiftFactor)
vecProbT

#Let's visually compare our control and treatment distrubitons

#We'll create a function to do this here that we'll call later as well
PlotDists <- function(vecProbC, vecProbT, strSubtitle='') { 

  dtPlot <- data.table('Response'=c(vecPolBias, vecPolBias),
                       'Assignment'=c(rep('Control',length(vecPolBias)),
                                      rep('Treatment',length(vecPolBias))),
                       'Probability'=c(vecProbC, vecProbT)
                       )
  
  ggplot(data=dtPlot, aes(x=Response, y=Probability, col=Assignment, group=Assignment)) +
   geom_line()+
   geom_point()+
   ggtitle(paste('Probability Mass Function\n',strSubtitle,sep='')) +
   theme(axis.text.x = element_text(angle = 90, hjust = 1))

}

PlotDists(vecProbC, vecProbT, 'Uniform (Control) vs. Uniform Shifted Response (Treatment)')

```

Having visualized our new treatment distribution, let's run our tests to see how frequently we detect effects.

```{r}
### UNIFORM SHIFTED DISTRIBUTION, POWER CURVES - LONG RUN TIME! ###

#Run a single test
intSampSize = 500
dt <- GenRandData(intSampSize, vecProbC, vecProbT)
m <- polr(as.factor(outcome) ~ treat, data = dt, Hess=TRUE)
summary(m)
tValue <- coef(summary(m))['treat', 't value']
tValue
pValue <- pnorm(abs(tValue), lower.tail=F) * 2
pValue
pValue < 0.05

#Now let's replicate it several times and see how many times it detects the difference.
intSampSize = 100
intNumReps <- 1000
pValsNull <- replicate(intNumReps, RepPVals(intSampSize, vecProbC, vecProbT, pThresh=0.05))
numDetected <- sum(pValsNull) / intNumReps
numDetected

#Finally, let's generate "power curves" from replicating this simulation with various
#effect sizes and N-sizes

vecSampSizeTrials <- c(10, 25, 50, 100, 250, 500, 1000)
vecShiftFactorTrials <- seq(5) / 50  #(0.02, 0.04, 0.06, 0.08, 0.10)

#Start a data table with just our sample sizes (we'll fill in the rest as the experiment churns)
dtResultsUniShift <- data.table('NumResponses'=vecSampSizeTrials)
dtResultsUniShift

#Poor man's Grid Search to generate results ...
for (numShiftFactor in vecShiftFactorTrials){
  vecResults <- c(0) #Just a dummy value
  
  for (intSampSize in vecSampSizeTrials){
    
    vecProbT <- vecProbC + c(-numShiftFactor,0,0,0,0,0,numShiftFactor)
    pValsNull <- replicate(intNumReps, RepPVals(intSampSize, vecProbC, vecProbT, pThresh=0.05))
    numDetected <- sum(pValsNull) / intNumReps
    vecResults <- c(vecResults, numDetected)
    
    #print(intSampSize)
    #print(numShiftFactor)
    #print(numDetected)
    
  }
  print(vecResults[-1:-1])
  dtResultsUniShift[, paste('ATE',numShiftFactor,sep='') := vecResults[-1:-1] ]
  
}

dtResultsUniShift
dtResultsUniShift #Not sure why, but it takes calling this twice to see the results

#Save results to disk
fwrite(dtResultsUniShift, 'dtResultsUniShift.csv')
```

## Uniform Tilted Response

Now let's look at a tilted response.

```{r}
### UNIFORM TILTED DISTRIBUTION GENERATION ###

#Uniform Tilted Responses
numMaxTiltFactor <- 0.05
vecProbT <- vecProbC + c(-numMaxTiltFactor,-numMaxTiltFactor*2/3,-numMaxTiltFactor*1/3,0,
                         numMaxTiltFactor*1/3,numMaxTiltFactor*2/3,numMaxTiltFactor)
vecProbT

#Visually compare our control and treatment distrubitons
PlotDists(vecProbC, vecProbT, 'Uniform (Control) vs. Uniform Shifted Response (Treatment)')

```

Now let's see how this tilted distribution runs in our power tests.

```{r}
### UNIFORM TILTED DISTRIBUTION, POWER CURVES - LONG RUN TIME! ###

#Run a single test
intSampSize = 500
dt <- GenRandData(intSampSize, vecProbC, vecProbT)
m <- polr(as.factor(outcome) ~ treat, data = dt, Hess=TRUE)
summary(m)
tValue <- coef(summary(m))['treat', 't value']
tValue
pValue <- pnorm(abs(tValue), lower.tail=F) * 2
pValue
pValue < 0.05

#Now let's replicate it several times and see how many times it detects the difference.
intSampSize = 100
intNumReps <- 1000
pValsNull <- replicate(intNumReps, RepPVals(intSampSize, vecProbC, vecProbT, pThresh=0.05))
numDetected <- sum(pValsNull) / intNumReps
numDetected

#Finally, let's generate "power curves" from replicating this simulation with various
#effect sizes and N-sizes

vecSampSizeTrials <- c(25, 50, 100, 250, 500, 1000)
#vecSampSizeTrials <- c(25, 50)
vecMaxTiltFactorTrials <- seq(5) / 50  #(0.02, 0.04, 0.06, 0.08, 0.10)
#vecMaxTiltFactorTrials <- seq(2) / 50  #(0.02, 0.04)

#Start a data table with just our sample sizes (we'll fill in the rest as the experiment churns)
dtResults <- data.table('NumResponses'=vecSampSizeTrials)
dtResults

#Poor man's Grid Search to generate results ...
for (numMaxTiltFactor in vecMaxTiltFactorTrials){
  vecResults <- c(0) #Just a dummy value

  for (intSampSize in vecSampSizeTrials){

    vecProbT <- vecProbC + c(-numMaxTiltFactor,-numMaxTiltFactor*2/3,-numMaxTiltFactor*1/3,0,
                             numMaxTiltFactor*1/3,numMaxTiltFactor*2/3,numMaxTiltFactor)
    pValsNull <- replicate(intNumReps, RepPVals(intSampSize, vecProbC, vecProbT, pThresh=0.05))
    numDetected <- sum(pValsNull) / intNumReps
    vecResults <- c(vecResults, numDetected)
    
  }
  print(vecResults[-1:-1])
  dtResults[, paste('ATE',numMaxTiltFactor,sep='') := vecResults[-1:-1] ]

}

dtResults
dtResults #Not sure why, but it takes calling this twice to see the results

#Save results to disk
fwrite(dtResults, 'dtResultsUniTilt.csv')
```

# Normally Distributed Responses

While the uniform distribution had the property of being totally random, that may not best mimic how actual users will respond. Instead, it's plausible that they'll cluster around a single response or group of responses, with a much lower chance of choosing other response options, which may be best illustrated with a normal distribution. Applying the treatment of opinion labels to subjects may then shift this distribution in one direction or the other. Here is what this will look like visually.

```{r}
### NORMAL SHIFTED DISTRIBUTION GENERATION ###

#First create the control group distribution
vecCenteredNums <- vecCategoryNums - 4 #Centers 'neutral' category at 0
numNormSpike <- 0.7  #Adjusts the 'spikiness' of the normal distribution, higher is spikier
numNormShift <- 1  #Adjusts the shift of the treatment group

#Quick function to normalize the probability mass to sum to 1
NormalizeMass <- function(distIn) {
  return(distIn/sum(distIn))
}

vecProbC <- NormalizeMass(dnorm((vecCenteredNums-0) * numNormSpike))
vecProbC

#Create the treatment group distribution
vecProbT <- NormalizeMass(dnorm((vecCenteredNums-numNormShift) * numNormSpike))
vecProbT

#Visually compare our control and treatment distrubitons
PlotDists(vecProbC, vecProbT, 'Normal (Control) vs. Normal Shifted Response (Treatment)')

```

Of note, the `numNormSpike` factor was set to 0.7 to give a small probability (as opposed to nearly 0) that the farthest outliers of Very Liberal and Very Conservative would occasionally be chosen.

Now that we see the distributions visually, we will calculate the Power probabilities at various effect sizes and sample sizes.

```{r}
### NORMAL SHIFTED DISTRIBUTION POWER CURVES - LONG RUN TIME! ###

#Run a single test
intSampSize = 500
dt <- GenRandData(intSampSize, vecProbC, vecProbT)
m <- polr(as.factor(outcome) ~ treat, data = dt, Hess=TRUE)
summary(m)
tValue <- coef(summary(m))['treat', 't value']
tValue
pValue <- pnorm(abs(tValue), lower.tail=F) * 2
pValue
pValue < 0.05

#Now let's replicate it several times and see how many times it detects the difference.
intSampSize = 100
intNumReps <- 1000
pValsNull <- replicate(intNumReps, RepPVals(intSampSize, vecProbC, vecProbT, pThresh=0.05))
numDetected <- sum(pValsNull) / intNumReps
numDetected

#Finally, let's generate "power curves" from replicating this simulation with various
#effect sizes and N-sizes

vecSampSizeTrials <- c(25, 50, 100, 250, 500, 1000)
#vecSampSizeTrials <- c(25, 50)
vecNormShiftTrials <- c(0.25, 0.5, 0.75, 1, 1.5, 2)
#vecNormShiftTrials <- c(0.25, 0.5)

#Start a data table with just our sample sizes (we'll fill in the rest as the experiment churns)
dtResults <- data.table('NumResponses'=vecSampSizeTrials)
dtResults

#Poor man's Grid Search to generate results ...
for (numNormShift in vecNormShiftTrials){
  vecResults <- c(0) #Just a dummy value

  for (intSampSize in vecSampSizeTrials){

    vecProbT <- NormalizeMass(dnorm((vecCenteredNums-numNormShift) * numNormSpike))
    pValsNull <- replicate(intNumReps, RepPVals(intSampSize, vecProbC, vecProbT, pThresh=0.05))
    numDetected <- sum(pValsNull) / intNumReps
    vecResults <- c(vecResults, numDetected)
    
  }
  print(vecResults[-1:-1])
  dtResults[, paste('ATE',numNormShift,sep='') := vecResults[-1:-1] ]

}

dtResults
dtResults #Not sure why, but it takes calling this twice to see the results

#Save results to disk
fwrite(dtResults, 'dtResultsNormShift.csv')
```

```{r}

```



